{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF original: (8528, 397)\n",
      "DF after dropna: (8528, 391)\n",
      "X:(8528, 390)\n",
      "X after imputer:(8528, 386)\n",
      "Train label distribution:\n",
      "Counter({'N': 4038, 'O': 1959, 'A': 592, '~': 233})\n",
      "\n",
      "Test label distribution:\n",
      "Counter({'N': 1012, 'O': 496, 'A': 147, '~': 51})\n",
      "\n",
      "Train dataset contains 6822 rows and 386 columns\n",
      "Test dataset contains 1706 rows and 386 columns\n",
      "F1 scores: [ 0.84750944  0.86793837  0.84849727  0.79283666  0.85404859]\n",
      "F1 mean: 0.842166064022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Imports\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', 500)\n",
    "seed = 1\n",
    "\n",
    "\n",
    "# ## Load dataset / Preprocessing\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_file = \"../data/ait_result_dataset.V38.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "print(\"DF original: \"+str(df.shape))\n",
    "#df=df.replace('NaN',np.nan)\n",
    "before=df.shape[1]\n",
    "df=df.dropna(axis=1, how='all')\n",
    "after=df.shape[1]\n",
    "#print(str(before-after)+ \" NaN columns dropped\")\n",
    "\n",
    "\n",
    "#df=df.dropna(axis=1, thresh=len(df) - 10)\n",
    "#df=df.dropna(axis=0, how='all')\n",
    "print(\"DF after dropna: \"+str(df.shape))\n",
    "df=df.replace(np.nan,0)\n",
    "#display(df)\n",
    "numpy_array = df.as_matrix()\n",
    "\n",
    "n_features=numpy_array.shape[1]\n",
    "X=numpy_array[:,0:n_features-1]\n",
    "y=numpy_array[:,n_features-1]\n",
    "\n",
    "#display(pd.DataFrame(X))\n",
    "#display(pd.DataFrame(y))\n",
    "\n",
    "print(\"X:\"+str(X.shape))\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "print(\"X after imputer:\"+str(X.shape))\n",
    "\n",
    "#display(pd.DataFrame(X))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(\"Train label distribution:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(Counter(y_test))\n",
    "\n",
    "print(\"\\nTrain dataset contains {0} rows and {1} columns\".format(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".format(X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "def decision_tree():\n",
    "    # ## Decision tree\n",
    "\n",
    "    # In[148]:\n",
    "\n",
    "    decision_tree = DecisionTreeClassifier(random_state=seed) #,class_weight=\"balanced\"\n",
    "\n",
    "    # train classifier\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    # predict output\n",
    "    decision_tree_y_pred  = decision_tree.predict(X_test)\n",
    "    decision_tree_y_pred_prob  = decision_tree.predict_proba(X_test)\n",
    "\n",
    "    # evaluation\n",
    "    decision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)\n",
    "    decision_tree_logloss = log_loss(y_test, decision_tree_y_pred_prob)\n",
    "\n",
    "    print(\"== Decision Tree ==\")\n",
    "    print(\"Accuracy: {0:.2f}\".format(decision_tree_accuracy))\n",
    "    print(\"Log loss: {0:.2f}\".format(decision_tree_logloss))\n",
    "    print(\"Number of nodes created: {}\".format(decision_tree.tree_.node_count))\n",
    "    print(\"F1: {:1.4f}\".format(f1_score(y_test, adaboost_y_pred,labels=['A','O','N'],average='macro')))  \n",
    "\n",
    "\n",
    "def adaboost():\n",
    "    # ## Adaboost\n",
    "\n",
    "    # In[91]:\n",
    "\n",
    "    decision_tree = DecisionTreeClassifier(random_state=seed)\n",
    "    adaboost = AdaBoostClassifier(\n",
    "        base_estimator=decision_tree,\n",
    "        algorithm='SAMME.R',\n",
    "        n_estimators=100,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.1)\n",
    "\n",
    "    # train classifier\n",
    "    adaboost.fit(X_train, y_train)\n",
    "\n",
    "    # calculate predictions\n",
    "    adaboost_y_pred = adaboost.predict(X_test)\n",
    "    adaboost_y_pred_prob = adaboost.predict_proba(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    adaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)\n",
    "    adaboost_logloss = log_loss(y_test, adaboost_y_pred_prob)\n",
    "\n",
    "    print(\"== AdaBoost ==\")\n",
    "    print(\"Accuracy: {0:.2f}\".format(adaboost_accuracy))\n",
    "    print(\"Log loss: {0:.2f}\".format(adaboost_logloss))\n",
    "    print(\"F1: {:1.4f}\".format(f1_score(y_test, adaboost_y_pred,labels=['A','O','N'],average='macro')))  \n",
    "\n",
    "\n",
    "def rf():\n",
    "    # ## Random Forest\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "    # Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=seed,n_estimators=100,class_weight=\"balanced\")\n",
    "\n",
    "    # Train the Classifier to take the training features and learn how they relate\n",
    "    # to the training y (the species)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # calculate predictions\n",
    "    rf_y_pred = rf.predict(X_test)\n",
    "    rf_y_pred_prob = rf.predict_proba(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "    rf_logloss = log_loss(y_test, rf_y_pred_prob)\n",
    "\n",
    "    print(\"== RandomForest ==\")\n",
    "    print(\"Accuracy: {0:.2f}\".format(rf_accuracy))\n",
    "    print(\"Log loss: {0:.2f}\".format(rf_logloss))\n",
    "    print(\"F1: {:1.4f}\".format(f1_score(y_test, rf_y_pred,labels=['A','O','N'],average='macro')))  \n",
    "\n",
    "\n",
    "\n",
    "def gbc():\n",
    "    # ## Gradient Boosting Classifier\n",
    "\n",
    "    # In[147]:\n",
    "\n",
    "    gbc = GradientBoostingClassifier(\n",
    "        max_depth=1,\n",
    "        n_estimators=100,\n",
    "        random_state=seed)\n",
    "    gbc.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    gbc_y_pred = gbc.predict(X_test)\n",
    "    gbc_y_pred_prob = gbc.predict_proba(X_test)\n",
    "\n",
    "    # calculate log loss\n",
    "    gbc_accuracy = accuracy_score(y_test, gbc_y_pred)\n",
    "    gbc_logloss = log_loss(y_test, gbc_y_pred_prob)\n",
    "\n",
    "    print(\"== Gradient Boosting ==\")\n",
    "    print(\"Accuracy: {0:.2f}\".format(gbc_accuracy))\n",
    "    print(\"Log loss: {0:.2f}\".format(gbc_logloss))\n",
    "    print(\"F1: {:1.4f}\".format(f1_score(y_test, gbc_y_pred,labels=['A','O','N'],average='macro')))  \n",
    "\n",
    "\n",
    "\n",
    "def xgboost():\n",
    "    # ## XGBoost\n",
    "\n",
    "    # In[183]:\n",
    "\n",
    "    #params = {\n",
    "    #    'objective': 'multi:softmax',\n",
    "    #    'max_depth': 7,\n",
    "    #    'learning_rate': 0.1,\n",
    "    #    'silent': 1,\n",
    "    #    'n_estimators': 200,\n",
    "    #    'nthread': 4,\n",
    "    #    'colsample_bytree': 0.8,\n",
    "    #    'gamma':0,\n",
    "    #       'subsample':1,\n",
    "    #       'min_child_weight':1\n",
    "    #}\n",
    "    #84.53,83.85 (cv)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'max_depth': 9,\n",
    "        'learning_rate': 0.10836619673151071,\n",
    "        'silent': 1,\n",
    "        'n_estimators': 200,\n",
    "        'nthread': 8,\n",
    "        'colsample_bytree': 0.27958200096140395,\n",
    "        'gamma':0.25,\n",
    "        'subsample':0.93166895175080411,\n",
    "        'min_child_weight':1,\n",
    "        'colsample_bylevel':1,\n",
    "        'max_delta_step':0,\n",
    "        'missing':None,\n",
    "        'reg_alpha':0\n",
    "    }\n",
    "    #84.33,\n",
    "    bst = XGBClassifier(**params)\n",
    "    #xgb_y_pred = bst.predict(X_test)\n",
    "\n",
    "    #print(\"F1: {:1.4f}\".format(f1_score(y_test, xgb_y_pred,labels=['A','O','N'],average='macro')))  \n",
    "\n",
    "    #display(pd.DataFrame(xgb_y_pred))\n",
    "    #correct = 0\n",
    "\n",
    "    #for i in range(len(xgb_y_pred)):\n",
    "    #    if (y_test[i] == xgb_y_pred[i]):\n",
    "    #        correct += 1\n",
    "    #acc = accuracy_score(y_test, xgb_y_pred)\n",
    "    #print('Predicted correctly: {0}/{1}'.format(correct, len(xgb_y_pred)))\n",
    "    #print('Error: {0:.4f}'.format(1-acc))\n",
    "    #joblib.dump(bst, 'xgboost.pkl', compress = 1)\n",
    "    scorer = make_scorer(f1_score, labels=['A','O','N'], average='macro')\n",
    "    scores = cross_val_score(bst, X, y, cv=5,scoring=scorer)\n",
    "    print(\"F1 scores:\",scores)\n",
    "    print(\"F1 mean:\",np.mean(scores))\n",
    "    joblib.dump(bst, 'xgboost.pkl', compress = 1)\n",
    "    joblib.dump(scores, 'scores.pkl', compress = 1)\n",
    "\n",
    "# In[4]:\n",
    "def gridSearch():\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    cv = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    " \n",
    "    params_grid = {\n",
    "       'gamma':[0,1,2],\n",
    "        'min_child_weight': [0.1,1,20],\n",
    "        'subsample':[0.8,0.9,1], \n",
    "        'colsample_bytree': [0.8,0.9,1]\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    params_fixed = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'silent': 1,\n",
    "        'max_depth': 7,\n",
    "        'n_estimators': 200,\n",
    "         'nthread': 4\n",
    "    }\n",
    "\n",
    "\n",
    "    scorer = make_scorer(f1_score, labels=['A','O','N'], average='macro')\n",
    "\n",
    "    bst_grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "        param_grid=params_grid,\n",
    "        cv=cv,\n",
    "        scoring=scorer\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    bst_grid.fit(X, y)\n",
    "    print(bst_grid.best_estimator_)\n",
    "    joblib.dump(bst_grid, 'grid.pkl', compress = 1)\n",
    "\n",
    "#gridSearch()\n",
    "xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.842166064022\n"
     ]
    }
   ],
   "source": [
    "scores=joblib.load('scores.pkl')\n",
    "print(\"F1:\",np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
